#!/usr/bin/env python3
"""
output ë””ë ‰í† ë¦¬ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- ì¤‘ë³µ íŒŒì¼ ì œê±° (_improved_improved.txt ë“±)
- ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼ ì •ë¦¬
- íŒŒì¼ í¬ê¸° ë¶„ì„
"""

import os
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict

OUTPUT_DIR = Path("output")
DRY_RUN = True  # ì‹¤ì œ ì‚­ì œ ì „ì— ë¯¸ë¦¬ë³´ê¸°

def analyze_output_directory():
    """output ë””ë ‰í† ë¦¬ ë¶„ì„"""
    files_by_type = defaultdict(list)
    total_size = 0
    
    if not OUTPUT_DIR.exists():
        print(f"âŒ {OUTPUT_DIR} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    for file_path in OUTPUT_DIR.rglob("*"):
        if file_path.is_file():
            size = file_path.stat().st_size
            total_size += size
            ext = file_path.suffix.lower()
            
            files_by_type[ext].append({
                "path": file_path,
                "size": size,
                "mtime": datetime.fromtimestamp(file_path.stat().st_mtime)
            })
    
    print("=" * 70)
    print("ğŸ“Š output ë””ë ‰í† ë¦¬ ë¶„ì„ ê²°ê³¼")
    print("=" * 70)
    print(f"\nì´ íŒŒì¼ ìˆ˜: {sum(len(files) for files in files_by_type.values())}ê°œ")
    print(f"ì´ í¬ê¸°: {total_size / 1024 / 1024:.2f} MB")
    
    print(f"\nğŸ“ íŒŒì¼ íƒ€ì…ë³„ í†µê³„:")
    for ext, files in sorted(files_by_type.items(), key=lambda x: len(x[1]), reverse=True):
        total_type_size = sum(f["size"] for f in files)
        print(f"  {ext or '(í™•ì¥ì ì—†ìŒ)':15} {len(files):4}ê°œ  {total_type_size / 1024 / 1024:7.2f} MB")
    
    return files_by_type, total_size

def find_duplicate_files(files_by_type):
    """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°"""
    duplicates = []
    
    # _improved_improved.txt íŒŒì¼ ì°¾ê¸°
    improved_improved = [
        f for f in files_by_type.get(".txt", [])
        if "_improved_improved" in f["path"].name
    ]
    
    if improved_improved:
        print(f"\nğŸ”„ ì¤‘ë³µ íŒŒì¼ ë°œê²¬:")
        print(f"  _improved_improved.txt: {len(improved_improved)}ê°œ")
        duplicates.extend(improved_improved)
    
    # ê°™ì€ ë² ì´ìŠ¤ëª…ì˜ ì—¬ëŸ¬ ë²„ì „ íŒŒì¼ ì°¾ê¸°
    base_files = defaultdict(list)
    for ext, files in files_by_type.items():
        for file_info in files:
            name = file_info["path"].stem
            # ë² ì´ìŠ¤ëª… ì¶”ì¶œ (ë²„ì „ ì œê±°)
            base = name.replace("_improved_improved", "").replace("_improved", "").replace("_script", "")
            base_files[base].append((ext, file_info))
    
    # ì—¬ëŸ¬ ë²„ì „ì´ ìˆëŠ” íŒŒì¼ ì°¾ê¸°
    multi_version = {base: files for base, files in base_files.items() if len(files) > 1}
    
    if multi_version:
        print(f"\nğŸ“¦ ì—¬ëŸ¬ ë²„ì „ íŒŒì¼:")
        for base, versions in list(multi_version.items())[:10]:
            print(f"  {base}:")
            for ext, file_info in versions:
                print(f"    - {file_info['path'].name} ({file_info['size'] / 1024:.1f} KB)")
    
    return duplicates, multi_version

def find_old_files(files_by_type, days=90):
    """ì˜¤ë˜ëœ íŒŒì¼ ì°¾ê¸°"""
    cutoff_date = datetime.now() - timedelta(days=days)
    old_files = []
    
    for files in files_by_type.values():
        for file_info in files:
            if file_info["mtime"] < cutoff_date:
                old_files.append(file_info)
    
    if old_files:
        print(f"\nâ° {days}ì¼ ì´ìƒ ëœ íŒŒì¼: {len(old_files)}ê°œ")
        for file_info in sorted(old_files, key=lambda x: x["mtime"])[:10]:
            age_days = (datetime.now() - file_info["mtime"]).days
            print(f"  - {file_info['path'].name} ({age_days}ì¼ ì „, {file_info['size'] / 1024:.1f} KB)")
    
    return old_files

def cleanup_files(duplicates, old_files, dry_run=True):
    """íŒŒì¼ ì •ë¦¬"""
    to_delete = []
    
    # ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ëŒ€ìƒ
    for file_info in duplicates:
        to_delete.append(file_info["path"])
    
    # ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼ ì‚­ì œ ëŒ€ìƒ (90ì¼ ì´ìƒ)
    for file_info in old_files:
        # ì¤‘ìš”í•œ íŒŒì¼ì€ ì œì™¸
        if file_info["path"].name.endswith("_audio_improved.mp3"):
            continue  # ì˜¤ë””ì˜¤ íŒŒì¼ì€ ë³´ì¡´
        if ".audio_generation_progress.json" in file_info["path"].name:
            continue  # ì§„í–‰ ìƒíƒœ íŒŒì¼ì€ ë³´ì¡´
        to_delete.append(file_info["path"])
    
    if not to_delete:
        print("\nâœ… ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    total_size = sum(f.stat().st_size for f in to_delete)
    print(f"\nğŸ—‘ï¸  ì‚­ì œ ëŒ€ìƒ: {len(to_delete)}ê°œ íŒŒì¼ ({total_size / 1024 / 1024:.2f} MB)")
    
    if dry_run:
        print("\nâš ï¸  DRY RUN ëª¨ë“œ - ì‹¤ì œë¡œ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("\nì‚­ì œë  íŒŒì¼ ëª©ë¡:")
        for file_path in to_delete[:20]:
            print(f"  - {file_path.name}")
        if len(to_delete) > 20:
            print(f"  ... ì™¸ {len(to_delete) - 20}ê°œ")
        print(f"\nğŸ’¡ ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´ DRY_RUN = Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        print("\nâš ï¸  ì‹¤ì œ ì‚­ì œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        deleted = 0
        for file_path in to_delete:
            try:
                file_path.unlink()
                deleted += 1
            except Exception as e:
                print(f"  âŒ {file_path.name} ì‚­ì œ ì‹¤íŒ¨: {e}")
        print(f"âœ… {deleted}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§¹ output ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹œì‘\n")
    
    # ë””ë ‰í† ë¦¬ ë¶„ì„
    files_by_type, total_size = analyze_output_directory()
    
    if not files_by_type:
        return
    
    # ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°
    duplicates, multi_version = find_duplicate_files(files_by_type)
    
    # ì˜¤ë˜ëœ íŒŒì¼ ì°¾ê¸°
    old_files = find_old_files(files_by_type, days=90)
    
    # íŒŒì¼ ì •ë¦¬
    cleanup_files(duplicates, old_files, dry_run=DRY_RUN)
    
    print("\n" + "=" * 70)
    print("âœ… ì •ë¦¬ ì™„ë£Œ")
    print("=" * 70)

if __name__ == "__main__":
    main()
